{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "month-wise_analysis.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "o4LPV7SffWzL"
      },
      "source": [
        "\n",
        "'''\n",
        "# The month wise labels generated in Exploration are evaluated here.\n",
        "# \n",
        "# Like % of hate, offense, on a month wise basis\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I_IklRfcdG2b"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M3a2r4F0dRx4"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import json\n",
        "import concurrent.futures\n",
        "import joblib\n",
        "import warnings\n",
        "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
        "\n",
        "import re\n",
        "import nltk"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk import word_tokenize,sent_tokenize\n",
        "from nltk.stem.porter import *\n",
        "\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Load the punkt tokenizer\n",
        "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "from wordcloud import WordCloud\n",
        "import matplotlib.colors as mcolors"
      ],
      "metadata": {
        "id": "GBKKz1SBcyBe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip3 install pickle5"
      ],
      "metadata": {
        "id": "1q_QPg2f4Zke"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_emojis(text):\n",
        "    emoj = re.compile(\"[\"\n",
        "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
        "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
        "        u\"\\U00002500-\\U00002BEF\"  # chinese char\n",
        "        u\"\\U00002702-\\U000027B0\"\n",
        "        u\"\\U00002702-\\U000027B0\"\n",
        "        u\"\\U000024C2-\\U0001F251\"\n",
        "        u\"\\U0001f926-\\U0001f937\"\n",
        "        u\"\\U00010000-\\U0010ffff\"\n",
        "        u\"\\u2640-\\u2642\" \n",
        "        u\"\\u2600-\\u2B55\"\n",
        "        u\"\\u200d\"\n",
        "        u\"\\u23cf\"\n",
        "        u\"\\u23e9\"\n",
        "        u\"\\u231a\"\n",
        "        u\"\\ufe0f\"  # dingbats\n",
        "        u\"\\u3030\"\n",
        "                      \"]+\", re.UNICODE)\n",
        "    return re.sub(emoj, '', text)\n",
        "\n",
        "def remove_hashtags(text):\n",
        "  return text.replace('#', '').replace('_', ' ')"
      ],
      "metadata": {
        "id": "kl0H1Ct_9lB5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess(text_string):\n",
        "    \"\"\"\n",
        "    Accepts a text string and replaces:\n",
        "    1) urls with URLHERE\n",
        "    2) lots of whitespace with one instance\n",
        "    3) mentions with MENTIONHERE\n",
        "\n",
        "    This allows us to get standardized counts of urls and mentions\n",
        "    Without caring about specific people mentioned\n",
        "    \"\"\"\n",
        "    space_pattern = '\\s+'\n",
        "    giant_url_regex = ('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|'\n",
        "        '[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')\n",
        "    mention_regex = '@[\\w\\-]+'\n",
        "    parsed_text = re.sub(space_pattern, ' ', text_string)\n",
        "    parsed_text = re.sub(giant_url_regex, '', parsed_text)\n",
        "    parsed_text = re.sub(mention_regex, '', parsed_text)\n",
        "    parsed_text = remove_hashtags(remove_emojis(parsed_text))\n",
        "    return parsed_text"
      ],
      "metadata": {
        "id": "GpXx82HqeFy2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pickle5\n",
        "import pickle5 as pickle\n",
        "\n",
        "with open('drive/MyDrive/dataframes/new_covid_pretrain_predictions.pkl', 'rb') as f:\n",
        "  df = pickle.load(f)\n",
        "# df = pd.read_pickle('drive/MyDrive/new_covid_pretrain_predictions.pkl')\n",
        "df.head(2)"
      ],
      "metadata": {
        "id": "O_O3exHOeMxJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.shape"
      ],
      "metadata": {
        "id": "ZdEHPDcLjoLc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls drive/MyDrive/dataframes/"
      ],
      "metadata": {
        "id": "bSUnrJRHK2Lh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "indices = random.sample(range(0,df[df.Label == 'Hate'].shape[0]), 25)\n",
        "df_sample_hate = df[df.Label == 'Hate'].iloc[indices][['TweetID', 'Tweet', 'Label']]\n",
        "indices = random.sample(range(0,df[df.Label == 'Neutral'].shape[0]), 475)\n",
        "df_sample_nonhate = df[df.Label == 'Neutral'].iloc[indices][['TweetID', 'Tweet', 'Label']]\n",
        "df_sample_500 = pd.concat([df_sample_hate, df_sample_nonhate], ignore_index=True)"
      ],
      "metadata": {
        "id": "pYjP1BRIN9t4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_sample_500 = pd.read_csv('drive/MyDrive/dataframes/df_sample_500.csv', index_col=0)"
      ],
      "metadata": {
        "id": "PyK8rv6EPMuq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "indices = random.sample(range(0,df[df.Label == 'Hate'].shape[0]), 500)\n",
        "df_sample_hate = df[df.Label == 'Hate'].iloc[indices][['TweetID', 'Tweet', 'Label']]\n",
        "for index, row in df_sample_hate.iterrows():\n",
        "  if row[0] in list(df_sample_500.TweetID):\n",
        "    df_sample_hate = df_sample_hate.drop(index)\n",
        "indices = random.sample(range(0,df_sample_hate[df_sample_hate.Label == 'Hate'].shape[0]), 475)\n",
        "df_sample_hate = df_sample_hate[df_sample_hate.Label == 'Hate'].iloc[indices][['TweetID', 'Tweet', 'Label']]\n",
        "\n",
        "indices = random.sample(range(0,df[df.Label == 'Neutral'].shape[0]), 500)\n",
        "df_sample_nonhate = df[df.Label == 'Neutral'].iloc[indices][['TweetID', 'Tweet', 'Label']]\n",
        "for index, row in df_sample_nonhate.iterrows():\n",
        "  if row[0] in list(df_sample_500.TweetID):\n",
        "    df_sample_nonhate = df_sample_nonhate.drop(index)\n",
        "indices = random.sample(range(0,df_sample_nonhate[df_sample_nonhate.Label == 'Neutral'].shape[0]), 25)\n",
        "df_sample_nonhate = df_sample_nonhate[df_sample_nonhate.Label == 'Neutral'].iloc[indices][['TweetID', 'Tweet', 'Label']]"
      ],
      "metadata": {
        "id": "1DfFApJSLHK7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_sample_500 = pd.concat([df_sample_hate, df_sample_nonhate], ignore_index=True)\n",
        "df_sample_500.Label.value_counts()"
      ],
      "metadata": {
        "id": "JaA4QrbpQE1w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_sample_500 = df_sample_500[['TweetID', 'Tweet', 'Label']].sample(frac=1).reset_index(drop=True)\n",
        "df_sample_500"
      ],
      "metadata": {
        "id": "8qvL8mytRDTi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_sample_500.to_csv('drive/MyDrive/dataframes/df_sample_500_2.csv')"
      ],
      "metadata": {
        "id": "pajyuuXwQSSi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_sample_500_task = df_sample_500[['TweetID', 'Tweet']]\n",
        "df_sample_500_task.to_csv('drive/MyDrive/dataframes/df_sample_500_2_task.csv') "
      ],
      "metadata": {
        "id": "k0fgcx0vRr2B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.Label.value_counts()"
      ],
      "metadata": {
        "id": "l7EusGY9h_t4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_len = df.shape[0]\n",
        "print('Whole data: ')\n",
        "print('Neutral: %.3f' % (df.Label.value_counts()['Neutral']/all_len))\n",
        "print('Other: %.3f' % (df.Label.value_counts()['Other']/all_len))\n",
        "print('Hate: %.3f' % (df.Label.value_counts()['Hate']/all_len))\n",
        "print('Counterhate: %.3f' % (df.Label.value_counts()['Counterhate']/all_len))\n",
        "\n",
        "# print('\\nSample: ')\n",
        "# print('Counterhate: %.3f' % (df_sample.Label.value_counts()['Neutral']/5000))\n",
        "# print('Other: %.3f' % (df_sample.Label.value_counts()['Other']/5000))\n",
        "# print('Hate: %.3f' % (df_sample.Label.value_counts()['Hate']/5000))\n",
        "# print('Counterhate: %.3f' % (df_sample.Label.value_counts()['Counterhate']/5000))"
      ],
      "metadata": {
        "id": "Km4bJm0TjaK-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_sample_500['Preprocessed'] = df_sample_500.Tweet.apply(preprocess)\n",
        "df_sample_500"
      ],
      "metadata": {
        "id": "K0GZ5SO-hdxV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_sample_500 = df_sample_500[['TweetID', 'Preprocessed', 'Label']].sample(frac=1).reset_index(drop=True)\n",
        "df_sample_500"
      ],
      "metadata": {
        "id": "wYzqdq2F-1cR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# df_sample_500.to_csv('drive/MyDrive/dataframes/df_sample_500.csv')\n",
        "df_sample_500 = pd.read_csv('drive/MyDrive/dataframes/df_sample_500.csv', index_col=0)"
      ],
      "metadata": {
        "id": "Loqc5VZjaxfR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_sample_500_anna.Label.value_counts()"
      ],
      "metadata": {
        "id": "z1TEnI-Lwg7j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def find_gt(id):\n",
        "  gt = df_sample_500.Label[df_sample_500.TweetID==id].values[0]\n",
        "  return 1 if gt == 'Hate' else 0"
      ],
      "metadata": {
        "id": "LGjeO_aiWkgl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_sample_500_anna = pd.read_csv('drive/MyDrive/dataframes/df_sample_500_task_anna.csv', index_col=0)\n",
        "df_sample_500_anna.head(2)"
      ],
      "metadata": {
        "id": "6RHet0qKPFUg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "find_gt(1268248943642763264)"
      ],
      "metadata": {
        "id": "IrWzNWr6XLwT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_sample_500_anna['GT'] = df_sample_500_anna.TweetID.apply(find_gt)"
      ],
      "metadata": {
        "id": "p-UpV6L8Vqsx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_sample_500_anna.GT.value_counts()"
      ],
      "metadata": {
        "id": "L3YnRhbHcEvf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_sample_500_anna.Label.value_counts()"
      ],
      "metadata": {
        "id": "gMqOdIVSdud8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def find_corrects(id):\n",
        "  l = df_sample_500_anna.Label[df_sample_500_anna.TweetID==id].values[0]\n",
        "  gt = df_sample_500_anna.GT[df_sample_500_anna.TweetID==id].values[0]\n",
        "  if l*gt ==1:\n",
        "    print(id)\n",
        "  return l*gt"
      ],
      "metadata": {
        "id": "0iKNKUvo0iTK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_sample_500_anna.Label[df_sample_500_anna.TweetID==df_sample_500_anna.iloc[110].TweetID].values[0]"
      ],
      "metadata": {
        "id": "kB-FV1s81BDq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_sample_500_anna['Corrects'] = df_sample_500_anna.TweetID.apply(find_corrects)\n",
        "df_sample_500_anna.head(10)"
      ],
      "metadata": {
        "id": "ad9A1ngwz5FD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_sample_500_anna.Corrects.value_counts()"
      ],
      "metadata": {
        "id": "eq8KUPrB1Vr-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "list(df_sample_500_anna[250:].Preprocessed[df_sample_500_anna.GT=='Hate'].values)"
      ],
      "metadata": {
        "id": "wUs-CHZPdjKP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_sample_500_task = df_sample_500[['TweetID', 'Preprocessed']].sample(frac=1).reset_index(drop=True)\n",
        "df_sample_500_task.to_csv('drive/MyDrive/dataframes/df_sample_500_task.csv')\n",
        "df_sample_500_task"
      ],
      "metadata": {
        "id": "luysQ9vFbPtb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "BdvhN-JGvss9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample_1000 = pd.read_csv('drive/MyDrive/dataframes/df_sample_task_1000.csv', index_col=0)\n",
        "sample_1000"
      ],
      "metadata": {
        "id": "l_zD-k9Nv-dt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "manual_labels = list(sample_1000[sample_1000.columns[2]].dropna().astype(int))"
      ],
      "metadata": {
        "id": "69jjZstIyEce"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "classifications_500_2 = pd.read_csv('drive/MyDrive/dataframes/df_sample_500_2.csv', index_col=0)\n",
        "classifications_500_2"
      ],
      "metadata": {
        "id": "kibqBix8x0Cp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "classifications = list(classifications_500_2.Label.map(lambda x: 1 if x=='Hate' else 0))"
      ],
      "metadata": {
        "id": "81okz0aaxwik"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "classifications.count(1)"
      ],
      "metadata": {
        "id": "e--C5soo26pm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "manual_labels.count(1)"
      ],
      "metadata": {
        "id": "OsZDyMHU3HE6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(manual_labels)"
      ],
      "metadata": {
        "id": "s5aD94403vJv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tp, tn, fp, fn = [0,0,0,0]\n",
        "for i in range(500):\n",
        "  if classifications[i] == manual_labels[500+i]:\n",
        "    if classifications[i] == 1:\n",
        "      tp += 1\n",
        "    else:\n",
        "      tn += 1\n",
        "  else:\n",
        "    if classifications[i] == 1:\n",
        "      fp += 1\n",
        "    else:\n",
        "      fn += 1\n",
        "tp, tn, fp, fn"
      ],
      "metadata": {
        "id": "8HrVxvsI0zsT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import inspect\n",
        "inspect.signature(metrics_calc)"
      ],
      "metadata": {
        "id": "jTmPO5MR1pH8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "metrics_calc(tn, fp, fn, tp)"
      ],
      "metadata": {
        "id": "FMTFxYTt2kQC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# MTurk HITs"
      ],
      "metadata": {
        "id": "DSs7Q_7evtYY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create rows with 15 tweets each (except for the last row)\n",
        "import math\n",
        "sample_size = 5012\n",
        "hit_size = 14\n",
        "rows = []\n",
        "for i in range(0, int(math.ceil(sample_size/hit_size))):\n",
        "  row = []\n",
        "  for j in range(0, hit_size):\n",
        "    if j+hit_size*i < sample_size:\n",
        "      tweet_tuple = list(df_selected_columns[['TweetID', 'Preprocessed', 'Label']].values[hit_size*i+j])\n",
        "      row.extend([str(tweet_tuple[0]), tweet_tuple[1], tweet_tuple[2]])\n",
        "  rows.append(row)"
      ],
      "metadata": {
        "id": "miALE0DrAd2j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_hits = pd.DataFrame(rows, columns=['id1', 'tweet1', 'label1', 'id2', 'tweet2', 'label2', 'id3', 'tweet3', 'label3', 'id4', 'tweet4', 'label4', 'id5', 'tweet5', 'label5', 'id6', 'tweet6', 'label6', 'id7', 'tweet7', 'label7', 'id8', 'tweet8', 'label8', 'id9', 'tweet9', 'label9', 'id10', 'tweet10', 'label10', 'id11', 'tweet11', 'label11', 'id12', 'tweet12', 'label12', 'id13', 'tweet13', 'label13', 'id14', 'tweet14', 'label14'])\n",
        "df_hits"
      ],
      "metadata": {
        "id": "Viqavv-rJP12"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_hits.to_csv('drive/MyDrive/dataframes/df_hits.csv', float_format=str)"
      ],
      "metadata": {
        "id": "tCYCRksvja47"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_hits_old = pd.read_csv('drive/MyDrive/df_hits.csv', index_col=0, float_precision='high')\n",
        "df_hits_old.head(2)"
      ],
      "metadata": {
        "id": "zHcynUa_yFry"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('drive/MyDrive/df_hits.csv', 'r') as f:\n",
        "  lines = f.readlines()[:21]\n",
        "print(lines[0])\n",
        "print(lines[1])"
      ],
      "metadata": {
        "id": "LP7JevxGpirN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ids = []\n",
        "for l in lines:\n",
        "  hit_ids  = []\n",
        "  for e in l.split(','):\n",
        "    if len(e) < 6:\n",
        "      continue\n",
        "    try:\n",
        "      hit_ids.append(int(e))\n",
        "    except:\n",
        "      pass\n",
        "  if len(hit_ids) > 0:\n",
        "    ids.append(hit_ids)"
      ],
      "metadata": {
        "id": "E-k13HnQzBuP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def find_label(id):\n",
        "  for index, row in df.iterrows():\n",
        "    if id == row[0]:\n",
        "      return row[-1]"
      ],
      "metadata": {
        "id": "S7XJ2o55z52v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "label_list = []\n",
        "for row in ids:\n",
        "  for id in row:\n",
        "    label_list.append([id, df.Label[df.TweetID == id].values[0]])"
      ],
      "metadata": {
        "id": "g9Zy3EKV2-eg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "label_list"
      ],
      "metadata": {
        "id": "5iE98cI032kk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_results = pd.read_csv('drive/MyDrive/20hits_result.csv')\n",
        "def find_answer(df):\n",
        "  annotations = []\n",
        "  hit = [[], [], []]\n",
        "  cnt = 0\n",
        "  for index, row in df.iterrows():\n",
        "    if cnt < 3:\n",
        "      for i in (list(range(55,79,2)) + [81, 83]):\n",
        "        if row[i] == True:\n",
        "          hit[cnt].append(1)\n",
        "        else:\n",
        "          hit[cnt].append(0)\n",
        "      cnt += 1\n",
        "    if cnt == 3:\n",
        "      total = [hit[0][j]+hit[1][j]+hit[2][j] for j in range(len(hit[0]))]\n",
        "      annotations.extend([1 if e>1 else 0 for e in total])\n",
        "      cnt = 0\n",
        "      hit = [[], [], []]\n",
        "  return annotations"
      ],
      "metadata": {
        "id": "CKo2e2_u3lX5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "majority_labels = find_answer(df_results)"
      ],
      "metadata": {
        "id": "XwquZwMM5v_N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "binary_label_list = [1 if e[1]=='Hate' else 0 for e in label_list]"
      ],
      "metadata": {
        "id": "hCSCD0wuBl9F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.array(majority_labels)"
      ],
      "metadata": {
        "id": "yY8aSHn0DYEs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.array(binary_label_list)"
      ],
      "metadata": {
        "id": "9zhes5vqD6gn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(len(majority_labels)):\n",
        "  if majority_labels[i] == 1:\n",
        "    print(i)"
      ],
      "metadata": {
        "id": "usLygY11FxxX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# with open('drive/MyDrive/Batch_4735221_batch_results_35hits.csv', 'r') as f:\n",
        "#   lines = f.readlines()\n",
        "df_35hits = pd.read_csv('drive/MyDrive/dataframes/Batch_4735221_batch_results_35hits.csv')\n",
        "df_35hits.head(2)"
      ],
      "metadata": {
        "id": "UdsWEUJzimP9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "headers = [\n",
        "  'Answer.Q1.1', 'Answer.Q2.1', 'Answer.Q3.1', 'Answer.Q4.1', 'Answer.Q5.1', \n",
        "  'Answer.Q6.1', 'Answer.Q8.1', 'Answer.Q9.1', 'Answer.Q10.1', 'Answer.Q11.1',\n",
        "  'Answer.Q12.1', 'Answer.Q13.1', 'Answer.Q14.1', 'Answer.Q15.1'\n",
        "]\n",
        "\n",
        "def find_answer_35hits(df):\n",
        "  annotations = []\n",
        "  hit = [[], [], []]\n",
        "  cnt = 0\n",
        "  for index, row in df.iterrows():\n",
        "    if cnt < 3:\n",
        "      for h in headers:\n",
        "        if row[h] == True:\n",
        "          hit[cnt].append(1)\n",
        "        else:\n",
        "          hit[cnt].append(0)\n",
        "      cnt += 1\n",
        "    if cnt == 3:\n",
        "      total = [hit[0][j]+hit[1][j]+hit[2][j] for j in range(len(hit[0]))]\n",
        "      annotations.extend([1 if e>2 else 0 for e in total])\n",
        "      cnt = 0\n",
        "      hit = [[], [], []]\n",
        "  return annotations"
      ],
      "metadata": {
        "id": "ALnx2RF2q1gA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "anns = find_answer_35hits(df_35hits)"
      ],
      "metadata": {
        "id": "6lo22m7XkSW-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(anns)"
      ],
      "metadata": {
        "id": "GDW-A_QckVcW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sum(anns)"
      ],
      "metadata": {
        "id": "kdVEXWmBsTbu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "labels = []\n",
        "cnt = 0\n",
        "for index, row in df_35hits.iterrows():\n",
        "  if cnt == 0:\n",
        "    for i in list(range(29,69,3)):\n",
        "      if row[i] == 'Hate':\n",
        "        labels.append(1)\n",
        "      elif row[i] == 'Neutral':\n",
        "        labels.append(0)\n",
        "      else:\n",
        "        print('Wrong content!')\n",
        "  cnt += 1\n",
        "  if cnt == 3:\n",
        "    cnt = 0"
      ],
      "metadata": {
        "id": "JpYVf4yZkkrS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tmp = [list(a) for a in df_35hits.iloc[::3, list(range(28,68,3))].values]\n",
        "tmp = [a[:] for a in tmp]\n",
        "tweets = []\n",
        "for i in range(len(tmp)):\n",
        "  tweets.extend(tmp[i])\n",
        "tweets"
      ],
      "metadata": {
        "id": "UJBqheyCDKBW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def metrics_calc(tn, fp, fn, tp):\n",
        "    cm = [int(tn), int(fp), int(fn), int(tp)]\n",
        "    recall = 0.0 if (tp+fn == 0) else float(format(tp/(tp+fn), '.4f'))\n",
        "    tpr = recall\n",
        "    tnr = 0.0 if (tn+fp == 0) else float(format(tn/(tn+fp), '.4f'))\n",
        "    fpr = 0.0 if (fp+tn == 0) else float(format(fp/(fp+tn), '.4f'))\n",
        "    fnr = 0.0 if (fn+tp == 0) else float(format(fn/(fn+tp), '.4f'))\n",
        "    accuracy = float(format((tp+tn)/(tn+fp+fn+tp), '.4f'))\n",
        "    precision = 0.0 if (tp+fp == 0) else float(format(tp/(tp+fp), '.4f'))\n",
        "    f1 = 0.0 if (precision+recall == 0.0) else \\\n",
        "                float(format((2*precision*recall)/(precision+recall), '.4f'))\n",
        "    print('tn, fp, fn, tp: ', cm)\n",
        "    print('tpr: ', tpr)\n",
        "    print('tnr: ', tnr)\n",
        "    print('fpr: ', fpr)\n",
        "    print('fnr: ', fnr)\n",
        "    print('accuracy: ', accuracy)\n",
        "    print('precision: ', precision)\n",
        "    print('recall: ', recall)\n",
        "    print('f1: ', f1)\n",
        "    return cm, tpr, tnr, fpr, fnr, accuracy, precision, recall, f1"
      ],
      "metadata": {
        "id": "x1-Te4w4Jylx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_sample_500_anna.Label.value_counts()"
      ],
      "metadata": {
        "id": "30yRdy0z2-wc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_sample_500_anna.GT.value_counts()"
      ],
      "metadata": {
        "id": "gTcfYlfl3Jh0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "metrics_calc(461, 20, 14, 5)"
      ],
      "metadata": {
        "id": "xm7IIBnv25-e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "tn, fp, fn, tp = confusion_matrix(anns, labels).ravel()\n",
        "metrics_calc(tn, fp, fn, tp)"
      ],
      "metadata": {
        "id": "eCDcb6o8mx4l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ground truth: \n",
        "\n",
        "490 total tweets\n",
        "56 hate classified\n",
        "\n",
        "434 non-hate\n",
        "5000"
      ],
      "metadata": {
        "id": "PYQgB9KdBGov"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fp = []\n",
        "fn = []\n",
        "for i in range(len(anns)):\n",
        "  if anns[i] != labels[i]:\n",
        "    if (anns[i]==True) and (labels[i]==False):\n",
        "      fn.append(tweets[i])\n",
        "    elif (anns[i]==False) and (labels[i]==True):\n",
        "      fp.append(tweets[i])"
      ],
      "metadata": {
        "id": "DHKWaaMVFYGx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_fn = pd.DataFrame(fn, columns=['tweet'])\n",
        "df_fn['status'] = 'FN'\n",
        "df_fn"
      ],
      "metadata": {
        "id": "MKNb4JxmGTv3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_fp = pd.DataFrame(fp, columns=['tweet'])\n",
        "df_fp['status'] = 'FP'\n",
        "df_fp"
      ],
      "metadata": {
        "id": "maE_9o5DG5Fc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pd.concat([df_fn, df_fp], ignore_index=True).to_csv('drive/MyDrive/dataframes/wrong-classifications.csv')"
      ],
      "metadata": {
        "id": "QZQf65eKHHbb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "aa=2;bb=5;cc=aa+bb\n",
        "print(f\"{aa} is better than {bb} because of {cc}\")"
      ],
      "metadata": {
        "id": "8gUd5lqsCUZa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Processing"
      ],
      "metadata": {
        "id": "6V3RiXcTv4Px"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YNsy5vh6fQmd"
      },
      "source": [
        "stopwords = nltk.corpus.stopwords.words(\"english\")\n",
        "other_exclusions = [\"#ff\", \"ff\", \"rt\"]\n",
        "stopwords.extend(other_exclusions)\n",
        "stemmer = PorterStemmer()\n",
        "wordnet_lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "def tokenize(tweet):\n",
        "    \"\"\"Removes punctuation & excess whitespace, sets to lowercase,\n",
        "    and stems tweets. Returns a list of stemmed tokens.\"\"\"\n",
        "    tweet = \" \".join(re.split(\"[^a-zA-Z]+\", tweet.lower())).strip()\n",
        "    tokens = [stemmer.stem(t) for t in tweet.split()]\n",
        "    return tokens\n",
        "\n",
        "def basic_tokenize(tweet):\n",
        "    \"\"\"Same as tokenize but without the stemming\"\"\"\n",
        "    tweet = \" \".join(re.split(\"[^a-zA-Z.,!?]+\", tweet.lower())).strip()\n",
        "    return tweet.split()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TArvoZRsfqmw"
      },
      "source": [
        "\n",
        "# import nltk\n",
        "# nltk.download('stopwords')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZXgiyvHGfkur"
      },
      "source": [
        "#Pre data\n",
        "\n",
        "df_pre = pd.read_pickle('/content/drive/My Drive/corona_hate_modelling/covid_raw_data/tweet_for_top_2k_pre_event.pkl')\n",
        "df_pre = df_pre[df_pre['Tweet_Type'] == \"tweet\"]\n",
        "\n",
        "df_pre[\"isTimeString\"] = df_pre['TimeStamp'].apply(lambda x : type(x)==type(1))\n",
        "df_int = df_pre[df_pre[\"isTimeString\"] == True]\n",
        "df_int['TimeStamp']  = pd.to_datetime(df_int['TimeStamp'],unit='s')\n",
        "\n",
        "df_str = df_pre[df_pre[\"isTimeString\"] == False]\n",
        "df_str['TimeStamp'] = pd.to_datetime(df_str['TimeStamp']).values.astype(np.int64) // 10 ** 9\n",
        "df_str['TimeStamp']  = pd.to_datetime(df_str['TimeStamp'],unit='s')\n",
        "\n",
        "df_pre = pd.concat([df_int, df_str])\n",
        "df_pre[\"Date\"] = df_pre[\"TimeStamp\"].apply(lambda x: x.date())\n",
        "\n",
        "df_pre['month'] = df_pre['TimeStamp'].dt.month\n",
        "df_pre.groupby('month')['TweetID'].count()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WLDxL_qL_8gN"
      },
      "source": [
        "len(df_pre[\"UserID\"].unique())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dF0i9Ue0fl12"
      },
      "source": [
        "#Post data\n",
        "df_post = pd.read_pickle('/content/drive/My Drive/corona_hate_modelling/covid_raw_data/tweet_for_top_2k_event.pkl')\n",
        "df_post = df_post[df_post['Tweet_Type'] == \"tweet\"]\n",
        "\n",
        "df_post[\"isTimeString\"] = df_post['TimeStamp'].apply(lambda x : type(x)==type(1))\n",
        "df_int = df_post[df_post[\"isTimeString\"] == True]\n",
        "df_int['TimeStamp']  = pd.to_datetime(df_int['TimeStamp'],unit='s')\n",
        "# df_int.head(2)\n",
        "\n",
        "\n",
        "df_str = df_post[df_post[\"isTimeString\"] == False]\n",
        "df_str['TimeStamp'] = pd.to_datetime(df_str['TimeStamp']).values.astype(np.int64) // 10 ** 9\n",
        "df_str['TimeStamp']  = pd.to_datetime(df_str['TimeStamp'],unit='s')\n",
        "# df_str.head()\n",
        "\n",
        "df_post = pd.concat([df_int, df_str])\n",
        "df_post[\"Date\"] = df_post[\"TimeStamp\"].apply(lambda x: x.date())\n",
        "\n",
        "df_post['month'] = df_post['TimeStamp'].dt.month\n",
        "df_post.groupby('month')['TweetID'].count()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZZX_ITYOB3xR"
      },
      "source": [
        "df = pd.concat([df_pre, df_post], axis=0)\n",
        "\n",
        "len(df[\"UserID\"].unique())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q1wpd2qRhcGM"
      },
      "source": [
        "pre_text = df_pre.Tweet\n",
        "corona_related_words = set()\n",
        "\n",
        "for tweet in pre_text:\n",
        "  p1 = preprocess(tweet)\n",
        "  p2 = basic_tokenize(p1)\n",
        "\n",
        "  for wrd in p2:\n",
        "    if wrd.lower().startswith('coron') or wrd.lower().startswith('viru') or wrd.lower().startswith('covid'):\n",
        "      corona_related_words.add(wrd)\n",
        "\n",
        "post_text = df_post.Tweet\n",
        "for tweet in post_text:\n",
        "  p1 = preprocess(tweet)\n",
        "  p2 = basic_tokenize(p1)\n",
        "\n",
        "  for wrd in p2:\n",
        "    if wrd.lower().startswith('coron') or wrd.lower().startswith('viru') or wrd.lower().startswith('covid'):\n",
        "      corona_related_words.add(wrd)\n",
        "\n",
        "corona_related_words.add('pandemic')\n",
        "\n",
        "with open('/content/drive/My Drive/corona_hate_modelling/covid_raw_data/corona_words.txt', 'w') as filehandle:\n",
        "    for listitem in corona_related_words:\n",
        "        filehandle.write('%s\\n' % listitem)\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lC7o3eFof-bw"
      },
      "source": [
        "# ## Function for plotting the word cloud\n",
        "import matplotlib as mpl\n",
        "def clusters_whole(df, month, stopwords_arg):\n",
        "    \n",
        "    month_tweets = df[df['month'] == month]['Tweet'].to_list()\n",
        "\n",
        "    clean_month_tweets = []\n",
        "    for i in month_tweets:\n",
        "        p1 = preprocess(i)\n",
        "        p2 = basic_tokenize(p1)\n",
        "        clean_month_tweets.append(\" \".join(p2))\n",
        "\n",
        "\n",
        "    mpl.rcParams['font.size']=12                #10 \n",
        "    mpl.rcParams['savefig.dpi']=100             #72 \n",
        "    mpl.rcParams['figure.subplot.bottom']=.1 \n",
        "\n",
        "\n",
        "    wordcloud = WordCloud(background_color='white',\n",
        "                              stopwords=stopwords_arg,\n",
        "                              max_words=200,\n",
        "                              max_font_size=40, \n",
        "                              random_state=42\n",
        "                             ).generate(\" \".join([i for i in clean_month_tweets]))\n",
        "\n",
        "#     print(new_corp)\n",
        "    fig = plt.figure(1)\n",
        "    plt.imshow(wordcloud)\n",
        "    plt.axis('off')\n",
        "    plt.show()\n",
        "    \n",
        "    return wordcloud\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nZBucu91pwnJ"
      },
      "source": [
        "corona_related_words = []\n",
        "\n",
        "# open file and read the content in a list\n",
        "with open('/content/drive/My Drive/corona_hate_modelling/covid_raw_data/corona_words.txt', 'r') as filehandle:\n",
        "    for line in filehandle:\n",
        "        # remove linebreak which is the last character of the string\n",
        "        currentPlace = line[:-1]\n",
        "\n",
        "        # add item to the list\n",
        "        corona_related_words.append(currentPlace)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1A3i_4IvpzhY"
      },
      "source": [
        "stopwords = list(stopwords) + list(corona_related_words)\n",
        "\n",
        "stopwords = set(stopwords)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EkMXebXop5LH"
      },
      "source": [
        "def get_pred_prec(y_pred):\n",
        "    \n",
        "    df = pd.DataFrame()\n",
        "    df['y_pred'] = y_pred\n",
        "    \n",
        "    return (df.groupby('y_pred')['y_pred'].count() / y_pred.shape[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4JFT7bezp8Yq"
      },
      "source": [
        "y_pred_dir = '/content/drive/My Drive/corona_hate_modelling/month_wise_data/post_event/'\n",
        "for month in sorted(df_post['month'].unique()):\n",
        "    \n",
        "    print (\"### Month ### \", month)\n",
        "    clusters_whole(df_post, month, stopwords)\n",
        "    \n",
        "    \n",
        "    y_pred = np.load(y_pred_dir + \"y_month{}.npy\".format(month))\n",
        "    print (get_pred_prec(y_pred))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yms7HphSqDo7"
      },
      "source": [
        "y_pred_dir = '/content/drive/My Drive/corona_hate_modelling/month_wise_data/pre_event/'\n",
        "for month in sorted(df_pre['month'].unique()):\n",
        "    \n",
        "    print (\"### Month ### \", month)\n",
        "    clusters_whole(df_pre, month, stopwords)\n",
        "    \n",
        "    \n",
        "    y_pred = np.load(y_pred_dir + \"y_month{}.npy\".format(month))\n",
        "    print (get_pred_prec(y_pred))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2JX470nOqjlr"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}